{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Data\n",
    "For both this part and the MySQL part below, the data was explored per column, either by printing column, using the function `pd.unique()`, or converting it to categorical variables and analysing the result. This was an easy way to find anomalies in the data and see what needed to be fixed. For dates and numerical values, I counted `NaN`s and `NaT`s using `isna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# read from csv\n",
    "df = pd.read_csv(\"BSCY4.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the Date column\n",
    "The Date column combines dates in three different formats:\n",
    "- `%Y-%m-%d`\n",
    "- `%d-%m-%Y` \n",
    "- `%d/%m` (where year must be fetched from 'year' column)\n",
    "\n",
    "To achieve consistency, all rows will be formatted according to the first type, as it contains the majority of the rows in the dataset. This is done by finding the indices of the rows with differently formatted dates, applying helper functions to format them, saving them to a new column called `temp`, and then combing this column with the original date column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to reorder dates of type d-m-Y\n",
    "# by switching year and day \n",
    "def order_date(d):\n",
    "    d[0], d[-1] = d[-1], d[0]\n",
    "    d = \"-\".join(d)\n",
    "    return d\n",
    "\n",
    "# helper function to reformat dates of type d/m by getting\n",
    "# the year and calling the order_date function\n",
    "def get_year(d, index):\n",
    "    year = df['year'][index]\n",
    "    d.append(str(year).rstrip('0').rstrip('.'))\n",
    "    d = order_date(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abnormalities in date: 338\n"
     ]
    }
   ],
   "source": [
    "# find rows with wrongly formatted dates. there are 338 in total.\n",
    "dates = pd.to_datetime(df['Date'], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "print(\"Abnormalities in date:\", dates.isna().sum())\n",
    "\n",
    "# get row indices \n",
    "# df.loc['Date']\n",
    "nat_indices = dates[dates.isnull() == True].index\n",
    "\n",
    "# create a new temporary column, then populate \n",
    "# it with adjusted date strings\n",
    "df['temp'] = pd.NaT\n",
    "\n",
    "for index in nat_indices:\n",
    "    date = df['Date'][index]\n",
    "    d = date.split('-')\n",
    "    \n",
    "    if len(d[-1]) == 4:\n",
    "        d = order_date(d)\n",
    "        df.at[index, 'temp'] = d\n",
    "    else:\n",
    "        d = date.split('/')\n",
    "        d = get_year(d, index)\n",
    "        df.at[index, 'temp'] = d\n",
    "\n",
    "# merge the date column with the temporary\n",
    "# column \n",
    "df['Date'] = dates\n",
    "df['Date'] = df['Date'].fillna(df['temp'])\n",
    "df.drop('temp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the Type column\n",
    "The Type column is categorical and contains 2 different types:\n",
    "- conventional\n",
    "- organic or Org.\n",
    "\n",
    "For consistency, any rows with 'Org.' will be reformatted to 'organic'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique categories in 'type' (before cleaning): ['conventional' 'organic' 'Org.']\n",
      "Abnormalities in 'type': 169\n"
     ]
    }
   ],
   "source": [
    "# replace 'Org.' with 'organic', then convert column\n",
    "# to type category. this gives us 2 categories in total.\n",
    "print(\"\\nUnique categories in 'type' (before cleaning):\", df['type'].unique())\n",
    "print(\"Abnormalities in 'type':\", df['type'].value_counts().to_dict()['Org.'])\n",
    "df['type'] = df['type'].astype(str).str.replace('Org.','organic')\n",
    "df['type'] = df['type'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the Average Price column\n",
    "The AveragePrice column is numeric and contains:\n",
    "- numbers with commas (e.g. 2,4)\n",
    "- numbers with decimal points (e.g. 2.4)\n",
    "- legitimate NaNs\n",
    "\n",
    "For consistency, commas will be replaced with decimal points and the NaNs will be kept (20 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legitimate NaNs in average price: 20\n",
      "Abnormalities in average price: 30\n"
     ]
    }
   ],
   "source": [
    "# replace commas with decimal points and cast to numeric \n",
    "errors = pd.to_numeric(df['AveragePrice'], errors='coerce')\n",
    "df['AveragePrice'] = df['AveragePrice'].astype(str).str.replace(',','.')\n",
    "df['AveragePrice'] = pd.to_numeric(df['AveragePrice'], errors=\"coerce\")\n",
    "\n",
    "# 20 legitimate nans in total\n",
    "print(\"Legitimate NaNs in average price:\", df['AveragePrice'].isna().sum())\n",
    "print(\"Abnormalities in average price:\", errors.isna().sum() - df['AveragePrice'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL Data\n",
    "A `pymysql` connection is made to read data from the 'Avocado' table in the database 'BSCY4' on localhost.\n",
    "\n",
    "## Handling the Region column\n",
    "Region is a categorical column and contains 57 regions in total before cleaning. \n",
    "Some region names have extra white spaces or dashes signifying a metroplex (e.g. 'Baltimore-Washington', which is the overlapping labor market region of the cities of Washington and Baltimore). For consistency, all white spaces and dashes are removed, giving a total of 54 **unique** categories.\n",
    "\n",
    "## Handling the Year column\n",
    "The year column contains years which are formatted in two different ways:\n",
    "- `yyyy` (e.g 2018)\n",
    "- `yy` (e.g 18) \n",
    "\n",
    "For consistency all dates will be formatted according to the former, less ambigious, type. The years will then be converted to `int64`.\n",
    "\n",
    "## Handling the Type column\n",
    "The type column contains a single category, with labels using different capitalisation ('Conventional' and 'conventional'). These labels will be formatted to lowercase and then converted to categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:········\n",
      "Abnormalities in region: 3\n",
      "Unique regions: 54\n",
      "Index(['Albany', 'Atlanta', 'BaltimoreWashington', 'Boise', 'Boston',\n",
      "       'BuffaloRochester', 'California', 'Charlotte', 'Chicago',\n",
      "       'CincinnatiDayton', 'Columbus', 'DallasFtWorth', 'Denver', 'Detroit',\n",
      "       'GrandRapids', 'GreatLakes', 'HarrisburgScranton',\n",
      "       'HartfordSpringfield', 'Houston', 'Indianapolis', 'Jacksonville',\n",
      "       'LasVegas', 'LosAngeles', 'Louisville', 'MiamiFtLauderdale', 'Midsouth',\n",
      "       'Nashville', 'NewOrleansMobile', 'NewYork', 'Northeast',\n",
      "       'NorthernNewEngland', 'Orlando', 'Philadelphia', 'PhoenixTucson',\n",
      "       'Pittsburgh', 'Plains', 'Portland', 'RaleighGreensboro',\n",
      "       'RichmondNorfolk', 'Roanoke', 'Sacramento', 'SanDiego', 'SanFrancisco',\n",
      "       'Seattle', 'SouthCarolina', 'SouthCentral', 'Southeast', 'Spokane',\n",
      "       'StLouis', 'Syracuse', 'Tampa', 'TotalUS', 'West', 'WestTexNewMexico'],\n",
      "      dtype='object')\n",
      "\n",
      "Abnormalities in year: 3208\n",
      "\n",
      "Unique categories in 'type' (before cleaning): ['conventional' 'Conventional']\n",
      "Abnormalities in 'type': 169\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import numpy as np\n",
    "import getpass\n",
    "\n",
    "con = pymysql.connect(host=\"127.0.0.1\", user=\"data_mining\", password=getpass.getpass(prompt='Password:'), database=\"BSCY4\")\n",
    "sql = pd.read_sql(\"SELECT * FROM AVOCADO\", con)\n",
    "\n",
    "# clean up duplicate categories and convert region \n",
    "# to categorical type \n",
    "errors = len(sql['region'].astype('category').cat.categories)\n",
    "sql['region'] = sql['region'].astype(str).str.strip().str.replace(\"-\", \"\")\n",
    "sql['region'] = sql['region'].astype('category')\n",
    "print(\"Abnormalities in region:\", errors - len(sql['region'].cat.categories))\n",
    "print(\"Unique regions:\", len(sql['region'].cat.categories))\n",
    "print(sql['region'].cat.categories)\n",
    "\n",
    "# fix year format and convert years to int64\n",
    "errors = len([x for x in sql['year'].astype(str) if len(x) == 2])\n",
    "print(\"\\nAbnormalities in year:\", errors)\n",
    "sql['year'] = sql['year'].astype(str)\n",
    "sql['year'] = sql.apply(lambda row: ('20' + row['year']) if len(row['year']) == 2 else row['year'], axis=1)\n",
    "sql['year'] = sql['year'].astype(np.int64)\n",
    "\n",
    "# fix capitalisation in type column and convert to\n",
    "# categorical variables\n",
    "print(\"\\nUnique categories in 'type' (before cleaning):\", sql['type'].unique())\n",
    "print(\"Abnormalities in 'type':\", sql['type'].value_counts().to_dict()['Conventional'])\n",
    "sql['type'] = sql['type'].astype(str).str.lower()\n",
    "sql['type'] = sql['type'].astype('category')\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating the data \n",
    "In order to concatenate the two dataframes, the column types and names need to be checked to make sure they are consistent. After this, a `pd.concat()` can be done to stack the dataframes on top of each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL columns: 13 \n",
      "CSV columns: 13\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "sql.head()\n",
    "\n",
    "# drop 'unamed' column from df, a column of the original indices  \n",
    "# which is not needed\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "print(\"SQL columns:\", len(sql.columns), \"\\nCSV columns:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types for CSV data:\n",
      " Date            datetime64[ns]\n",
      "AveragePrice           float64\n",
      "Total Volume           float64\n",
      "4046                   float64\n",
      "4225                   float64\n",
      "4770                   float64\n",
      "Total Bags             float64\n",
      "Small Bags             float64\n",
      "Large Bags             float64\n",
      "XLarge Bags            float64\n",
      "type                  category\n",
      "year                   float64\n",
      "region                  object\n",
      "dtype: object\n",
      "\n",
      "Data types for SQL data:\n",
      " Date              object\n",
      "AveragePrice     float64\n",
      "TotalValue       float64\n",
      "c4046            float64\n",
      "c4225            float64\n",
      "c4770            float64\n",
      "TotalBags        float64\n",
      "SmallBags        float64\n",
      "LargeBags        float64\n",
      "XLargeBags       float64\n",
      "type            category\n",
      "year               int64\n",
      "region          category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types for CSV data:\\n\", df.dtypes)\n",
    "print(\"\\nData types for SQL data:\\n\", sql.dtypes)\n",
    "\n",
    "# sql date needs to be converted to datetime\n",
    "sql['Date'] = pd.to_datetime(sql['Date'], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# csv region + year need to be converted\n",
    "df['year'] = df['year'].astype(np.int64)\n",
    "df['region'] = df['region'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "sql.head()\n",
    "\n",
    "# column names for df + sql are different, need to \n",
    "# make them consistent before merging\n",
    "column_names = {\n",
    "    'TotalValue':'Total Volume', \n",
    "    'c4046': '4046', \n",
    "    'c4225': '4225', \n",
    "    'c4770': '4770', \n",
    "    'TotalBags': 'Total Bags',\n",
    "    'SmallBags': 'Small Bags',\n",
    "    'LargeBags': 'Large Bags',\n",
    "    'XLargeBags': 'XLarge Bags'\n",
    "}\n",
    "\n",
    "# concatenate the data frames\n",
    "new_df = pd.concat([df, sql.rename(columns=column_names)], ignore_index=True)\n",
    "\n",
    "# need to cast the type column to categorical again. since the sql df\n",
    "# only had 1 category, the concatenation results in an 'object' type\n",
    "new_df['type'] = new_df['type'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types for concatenated data:\n",
      " Date            datetime64[ns]\n",
      "AveragePrice           float64\n",
      "Total Volume           float64\n",
      "4046                   float64\n",
      "4225                   float64\n",
      "4770                   float64\n",
      "Total Bags             float64\n",
      "Small Bags             float64\n",
      "Large Bags             float64\n",
      "XLarge Bags            float64\n",
      "type                  category\n",
      "year                     int64\n",
      "region                category\n",
      "dtype: object\n",
      "\n",
      "Total number of columns:\n",
      " 13\n",
      "\n",
      "Total number of rows:\n",
      " 18249\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData types for concatenated data:\\n\", new_df.dtypes)\n",
    "print(\"\\nTotal number of columns:\\n\", len(new_df.columns))\n",
    "print(\"\\nTotal number of rows:\\n\", len(new_df.index))\n",
    "\n",
    "new_df.to_csv(\"new_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way of looking through year (for reference)\n",
    "for i, item in df['year'].iteritems():\n",
    "    if len(item) == 2: \n",
    "        df.iloc[i, df.columns.get_loc('year')] = '20' + item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
